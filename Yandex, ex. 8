Яндекс.Практикум, задача 8: защитить данные клиентов страховой компании «Хоть потоп». 
Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию

### Оглавление:

0. Подгрузка необходимых библиотек.
1. Осмотр и обработка данных. 
2. Создание алгоритма на основе матричного шифрования для защиты данных.
3. Теоретическое подтверждение качества работы алгоритма. 
4. Вывод.

#### 0: Подгрузка необходимых библиотек.

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import matplotlib
pd.options.mode.chained_assignment = None

try:
    data = pd.read_csv('/Users/artemdudkin/Desktop/Работы Юпитер/Проект 8 - линейная алгебра/insurance.csv')
except:
    data = pd.read_csv('/datasets/insurance.csv')
    
    data.info() # пропусков и nan нету

# переименовываем столбцы
data_new_columns = ('gender', 'age', 'income', 'family', 'ins_pay')
data.columns=data_new_columns
Пропуски отсутствуют, сами данные имеют верный формат. Можно конвертировать в 32-формат для экономии памяти. 

#### 2: Создание алгоритма на основе матричного шифрования для защиты данных.

Для решения задачи я выбрал алгоритм матричного шифрования, когда строка признаков умножается на обратимую матрицу-ключ. 
С его же помощью она расшифровывается.
# формируем выборки
y = data['ins_pay']
x = data.drop('ins_pay', axis=1)
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=12345)

# масштабируем признаки
columns = ['gender', 'age', 'income', 'family']
scaler = StandardScaler() 
scaler.fit(x_train[columns])
x_train[columns] = scaler.transform(x_train[columns])
x_test[columns] = scaler.transform(x_test[columns])

# обучаем модель
model = LinearRegression()
model.fit(x_train,y_train)
print('Эффективность модели:', r2_score(y_test, model.predict(x_test)))

# зашифровка
x.head(5)

# создаем матрицу-ключ: 
key = np.round(np.random.normal(2,3, size=(4,4)))
# проверка на обратимость: 
np.linalg.inv(key) # нет ошибки, матрица обратима

# создание нового зашифрованного фрейма и проверка точности модели:
x_new = []
for i in range(0,5000): 
    x_new.append(key@x.loc[i].values)
x_new = pd.DataFrame(x_new, columns=('gender', 'age', 'income', 'family'))
x_new_train,x_new_test = train_test_split(x_new, test_size=0.2, random_state=12345)
model = LinearRegression()
model.fit(x_new_train, y_train)
r2_score(y_test, model.predict(x_new_test)) # точность изменилась на миллионные доли процента

# расшифровка
key_determ = np.linalg.inv(key)
x_old_new = []

for i in range(0,5000):
    x_old_new.append(key_determ@x_new.loc[i].values)
    
x_old_new = pd.DataFrame(x_old_new, columns=('gender', 'age', 'income', 'family')) # формируем новый старый фрейм  

np.round(mean_squared_error(x['gender'], x_old_new['gender']), 4) # количество ошибок стремится к нулю
np.round(mean_squared_error(x['family'], x_old_new['family']), 4) # количество ошибок стремится к нулю

Можно говорить, что после шифрования и до шифрования точность работы модели одинакова, а значит - алгоритм работает.

#### 3: Теоретическое подтверждение качества работы алгоритма. 
Основной теоретический вопрос, поставленный в работе, звучит так:
    
    при умножении признаков на обратимую матрицу, изменится ли качество линейной регрессии?
  
С практической точки зрения мы уже убедились, что не изменится. Произведем расчеты с точки зрения теории:  

Формула линейной регрессии имеет следующий вид: 

    a = (x,w) + w0
    где x - вектор признаков, w - вектор весов, w0 - величина сдвига предсказания
    
Рассматривать будем случай, когда у нас всего один признак X, тогда формула будет иметь вид: 

    a = X * w
    где w:
    w = (X.T @ X)** (−1) @ X.T * y

где:

    X = матрица значений
    w = вектор весов
    Р = некая обратимая матрица, на которую умножаются признаки

Код для юпитера:
Известно, что: 
        
$    a = Xw$

$    w = ((XP)^T(XP))^{−1}(XP)^T y$

совмещаем уравнения и перемножаем признаки на обратимую Р-матрицу: 

a1 = $ XP ((XP)^T(XP))^{−1} (XP)^T y $

Производим следующие операции:

1. a1 = $ XP((XP)^T(XP))^{−1} (XP)^T y $

2. a1 = $ XPP^{-1}((XP)^{T}X)^{-1} (XP)^Ty $

3. a1 = $ XPP^{-1}((XP)^{T}X)^{-1} P^TX^Ty $

4. a1 = $ XPP^{-1}(P^TX^TX)^{-1} P^TX^Ty $

5. a1 = $ XPP^{-1}(X^TX)^{-1}(P^T)^{-1} P^TX^Ty $

6. a1 = $ XPP^{-1}(X^TX)^{-1} X^Ty$

7. a1 = $ XPP^{-1}w $

8. a1 = $ Xw $

9. a1 = a

Видим, что значение неизменно.

#### 4: Вывод.
- Заказчиком были предоставлены корректно обработанные данные без пропусков, но с некорректно оформленными названиями признаков;
- Практически доказано, что можно зашифровать данные матричным методом с использованием матрицы-ключа, при условии, что эта матрица обратима;
- Потерь в качестве обучения модели линейной регрессии фактически нет, так как различия в качестве работы моделей на шифрованных и не шифрованных данных измеряются в миллионных долях процента;
- Теоретически подтверждено, что для любой обратимой матрицы верна гипотеза, что умножение признаков на обратимую матрицу не ведет к изменениям в работе модели.
